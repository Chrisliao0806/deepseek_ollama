import re
import ollama
from ollama import AsyncClient


async def ollama_llm(
    question, context=None, model_name="deepseek-r1:14b", remove_think=False
):
    """
    Asynchronously interacts with the Ollama language model to generate a response 
    based on the given question and context.

    Args:
        question (str): The question to ask the language model.
        context (str or None): Additional context to provide to the language model. If None, only the question is used.
        model_name (str, optional): The name of the language model to use. Defaults to "deepseek-r1:14b".
        remove_think (bool, optional): If True, removes content between <think> and </think> tags from the response. Defaults to False.

    Returns:
        str: The response generated by the language model.
    """
    if context is None:
        formatted_prompt = f"Question: {question}"
    else:
        formatted_prompt = f"Question: {question}\n\nContext: {context}"

    response = await AsyncClient().chat(
        model=model_name,
        messages=[{"role": "user", "content": formatted_prompt}],
    )
    if remove_think:
        # Remove content between <think> and </think> tags to remove thinking output
        response_content = re.sub(
            r"<think>.*?</think>", "", response["message"]["content"], flags=re.DOTALL
        ).strip()
    else:
        response_content = response["message"]["content"]

    return response_content
